{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "from modules.lm_head import LMHeadModel\n",
    "from modules.modeling_phi import PhiForCausalLM\n",
    "from utils.config import Config\n",
    "teacher_model = PhiForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-1.5\", attn_implementation=\"eager\"\n",
    ").to(device)\n",
    "teacher_model.eval()\n",
    "teacher_model.requires_grad_(False)\n",
    "\n",
    "dataset = load_dataset(\"stas/openwebtext-10k\")[\"train\"]\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, torch.nn.LayerNorm):\n",
    "        torch.nn.init.ones_(module.weight)\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, torch.nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.57it/s]\n",
      "Some weights of the model checkpoint at /common-repos/xLSTM-7b were not used when initializing xLSTMForCausalLM: ['backbone.blocks.16.ffn.proj_down.weight', 'backbone.blocks.16.ffn.proj_up.weight', 'backbone.blocks.16.ffn.proj_up_gate.weight', 'backbone.blocks.16.mlstm_layer.fgate_preact.bias', 'backbone.blocks.16.mlstm_layer.fgate_preact.weight', 'backbone.blocks.16.mlstm_layer.igate_preact.bias', 'backbone.blocks.16.mlstm_layer.igate_preact.weight', 'backbone.blocks.16.mlstm_layer.k.weight', 'backbone.blocks.16.mlstm_layer.multihead_norm.weight', 'backbone.blocks.16.mlstm_layer.ogate_preact.weight', 'backbone.blocks.16.mlstm_layer.out_proj.weight', 'backbone.blocks.16.mlstm_layer.q.weight', 'backbone.blocks.16.mlstm_layer.v.weight', 'backbone.blocks.16.norm_ffn.weight', 'backbone.blocks.16.norm_mlstm.weight', 'backbone.blocks.17.ffn.proj_down.weight', 'backbone.blocks.17.ffn.proj_up.weight', 'backbone.blocks.17.ffn.proj_up_gate.weight', 'backbone.blocks.17.mlstm_layer.fgate_preact.bias', 'backbone.blocks.17.mlstm_layer.fgate_preact.weight', 'backbone.blocks.17.mlstm_layer.igate_preact.bias', 'backbone.blocks.17.mlstm_layer.igate_preact.weight', 'backbone.blocks.17.mlstm_layer.k.weight', 'backbone.blocks.17.mlstm_layer.multihead_norm.weight', 'backbone.blocks.17.mlstm_layer.ogate_preact.weight', 'backbone.blocks.17.mlstm_layer.out_proj.weight', 'backbone.blocks.17.mlstm_layer.q.weight', 'backbone.blocks.17.mlstm_layer.v.weight', 'backbone.blocks.17.norm_ffn.weight', 'backbone.blocks.17.norm_mlstm.weight', 'backbone.blocks.18.ffn.proj_down.weight', 'backbone.blocks.18.ffn.proj_up.weight', 'backbone.blocks.18.ffn.proj_up_gate.weight', 'backbone.blocks.18.mlstm_layer.fgate_preact.bias', 'backbone.blocks.18.mlstm_layer.fgate_preact.weight', 'backbone.blocks.18.mlstm_layer.igate_preact.bias', 'backbone.blocks.18.mlstm_layer.igate_preact.weight', 'backbone.blocks.18.mlstm_layer.k.weight', 'backbone.blocks.18.mlstm_layer.multihead_norm.weight', 'backbone.blocks.18.mlstm_layer.ogate_preact.weight', 'backbone.blocks.18.mlstm_layer.out_proj.weight', 'backbone.blocks.18.mlstm_layer.q.weight', 'backbone.blocks.18.mlstm_layer.v.weight', 'backbone.blocks.18.norm_ffn.weight', 'backbone.blocks.18.norm_mlstm.weight', 'backbone.blocks.19.ffn.proj_down.weight', 'backbone.blocks.19.ffn.proj_up.weight', 'backbone.blocks.19.ffn.proj_up_gate.weight', 'backbone.blocks.19.mlstm_layer.fgate_preact.bias', 'backbone.blocks.19.mlstm_layer.fgate_preact.weight', 'backbone.blocks.19.mlstm_layer.igate_preact.bias', 'backbone.blocks.19.mlstm_layer.igate_preact.weight', 'backbone.blocks.19.mlstm_layer.k.weight', 'backbone.blocks.19.mlstm_layer.multihead_norm.weight', 'backbone.blocks.19.mlstm_layer.ogate_preact.weight', 'backbone.blocks.19.mlstm_layer.out_proj.weight', 'backbone.blocks.19.mlstm_layer.q.weight', 'backbone.blocks.19.mlstm_layer.v.weight', 'backbone.blocks.19.norm_ffn.weight', 'backbone.blocks.19.norm_mlstm.weight', 'backbone.blocks.20.ffn.proj_down.weight', 'backbone.blocks.20.ffn.proj_up.weight', 'backbone.blocks.20.ffn.proj_up_gate.weight', 'backbone.blocks.20.mlstm_layer.fgate_preact.bias', 'backbone.blocks.20.mlstm_layer.fgate_preact.weight', 'backbone.blocks.20.mlstm_layer.igate_preact.bias', 'backbone.blocks.20.mlstm_layer.igate_preact.weight', 'backbone.blocks.20.mlstm_layer.k.weight', 'backbone.blocks.20.mlstm_layer.multihead_norm.weight', 'backbone.blocks.20.mlstm_layer.ogate_preact.weight', 'backbone.blocks.20.mlstm_layer.out_proj.weight', 'backbone.blocks.20.mlstm_layer.q.weight', 'backbone.blocks.20.mlstm_layer.v.weight', 'backbone.blocks.20.norm_ffn.weight', 'backbone.blocks.20.norm_mlstm.weight', 'backbone.blocks.21.ffn.proj_down.weight', 'backbone.blocks.21.ffn.proj_up.weight', 'backbone.blocks.21.ffn.proj_up_gate.weight', 'backbone.blocks.21.mlstm_layer.fgate_preact.bias', 'backbone.blocks.21.mlstm_layer.fgate_preact.weight', 'backbone.blocks.21.mlstm_layer.igate_preact.bias', 'backbone.blocks.21.mlstm_layer.igate_preact.weight', 'backbone.blocks.21.mlstm_layer.k.weight', 'backbone.blocks.21.mlstm_layer.multihead_norm.weight', 'backbone.blocks.21.mlstm_layer.ogate_preact.weight', 'backbone.blocks.21.mlstm_layer.out_proj.weight', 'backbone.blocks.21.mlstm_layer.q.weight', 'backbone.blocks.21.mlstm_layer.v.weight', 'backbone.blocks.21.norm_ffn.weight', 'backbone.blocks.21.norm_mlstm.weight', 'backbone.blocks.22.ffn.proj_down.weight', 'backbone.blocks.22.ffn.proj_up.weight', 'backbone.blocks.22.ffn.proj_up_gate.weight', 'backbone.blocks.22.mlstm_layer.fgate_preact.bias', 'backbone.blocks.22.mlstm_layer.fgate_preact.weight', 'backbone.blocks.22.mlstm_layer.igate_preact.bias', 'backbone.blocks.22.mlstm_layer.igate_preact.weight', 'backbone.blocks.22.mlstm_layer.k.weight', 'backbone.blocks.22.mlstm_layer.multihead_norm.weight', 'backbone.blocks.22.mlstm_layer.ogate_preact.weight', 'backbone.blocks.22.mlstm_layer.out_proj.weight', 'backbone.blocks.22.mlstm_layer.q.weight', 'backbone.blocks.22.mlstm_layer.v.weight', 'backbone.blocks.22.norm_ffn.weight', 'backbone.blocks.22.norm_mlstm.weight', 'backbone.blocks.23.ffn.proj_down.weight', 'backbone.blocks.23.ffn.proj_up.weight', 'backbone.blocks.23.ffn.proj_up_gate.weight', 'backbone.blocks.23.mlstm_layer.fgate_preact.bias', 'backbone.blocks.23.mlstm_layer.fgate_preact.weight', 'backbone.blocks.23.mlstm_layer.igate_preact.bias', 'backbone.blocks.23.mlstm_layer.igate_preact.weight', 'backbone.blocks.23.mlstm_layer.k.weight', 'backbone.blocks.23.mlstm_layer.multihead_norm.weight', 'backbone.blocks.23.mlstm_layer.ogate_preact.weight', 'backbone.blocks.23.mlstm_layer.out_proj.weight', 'backbone.blocks.23.mlstm_layer.q.weight', 'backbone.blocks.23.mlstm_layer.v.weight', 'backbone.blocks.23.norm_ffn.weight', 'backbone.blocks.23.norm_mlstm.weight', 'backbone.blocks.24.ffn.proj_down.weight', 'backbone.blocks.24.ffn.proj_up.weight', 'backbone.blocks.24.ffn.proj_up_gate.weight', 'backbone.blocks.24.mlstm_layer.fgate_preact.bias', 'backbone.blocks.24.mlstm_layer.fgate_preact.weight', 'backbone.blocks.24.mlstm_layer.igate_preact.bias', 'backbone.blocks.24.mlstm_layer.igate_preact.weight', 'backbone.blocks.24.mlstm_layer.k.weight', 'backbone.blocks.24.mlstm_layer.multihead_norm.weight', 'backbone.blocks.24.mlstm_layer.ogate_preact.weight', 'backbone.blocks.24.mlstm_layer.out_proj.weight', 'backbone.blocks.24.mlstm_layer.q.weight', 'backbone.blocks.24.mlstm_layer.v.weight', 'backbone.blocks.24.norm_ffn.weight', 'backbone.blocks.24.norm_mlstm.weight', 'backbone.blocks.25.ffn.proj_down.weight', 'backbone.blocks.25.ffn.proj_up.weight', 'backbone.blocks.25.ffn.proj_up_gate.weight', 'backbone.blocks.25.mlstm_layer.fgate_preact.bias', 'backbone.blocks.25.mlstm_layer.fgate_preact.weight', 'backbone.blocks.25.mlstm_layer.igate_preact.bias', 'backbone.blocks.25.mlstm_layer.igate_preact.weight', 'backbone.blocks.25.mlstm_layer.k.weight', 'backbone.blocks.25.mlstm_layer.multihead_norm.weight', 'backbone.blocks.25.mlstm_layer.ogate_preact.weight', 'backbone.blocks.25.mlstm_layer.out_proj.weight', 'backbone.blocks.25.mlstm_layer.q.weight', 'backbone.blocks.25.mlstm_layer.v.weight', 'backbone.blocks.25.norm_ffn.weight', 'backbone.blocks.25.norm_mlstm.weight', 'backbone.blocks.26.ffn.proj_down.weight', 'backbone.blocks.26.ffn.proj_up.weight', 'backbone.blocks.26.ffn.proj_up_gate.weight', 'backbone.blocks.26.mlstm_layer.fgate_preact.bias', 'backbone.blocks.26.mlstm_layer.fgate_preact.weight', 'backbone.blocks.26.mlstm_layer.igate_preact.bias', 'backbone.blocks.26.mlstm_layer.igate_preact.weight', 'backbone.blocks.26.mlstm_layer.k.weight', 'backbone.blocks.26.mlstm_layer.multihead_norm.weight', 'backbone.blocks.26.mlstm_layer.ogate_preact.weight', 'backbone.blocks.26.mlstm_layer.out_proj.weight', 'backbone.blocks.26.mlstm_layer.q.weight', 'backbone.blocks.26.mlstm_layer.v.weight', 'backbone.blocks.26.norm_ffn.weight', 'backbone.blocks.26.norm_mlstm.weight', 'backbone.blocks.27.ffn.proj_down.weight', 'backbone.blocks.27.ffn.proj_up.weight', 'backbone.blocks.27.ffn.proj_up_gate.weight', 'backbone.blocks.27.mlstm_layer.fgate_preact.bias', 'backbone.blocks.27.mlstm_layer.fgate_preact.weight', 'backbone.blocks.27.mlstm_layer.igate_preact.bias', 'backbone.blocks.27.mlstm_layer.igate_preact.weight', 'backbone.blocks.27.mlstm_layer.k.weight', 'backbone.blocks.27.mlstm_layer.multihead_norm.weight', 'backbone.blocks.27.mlstm_layer.ogate_preact.weight', 'backbone.blocks.27.mlstm_layer.out_proj.weight', 'backbone.blocks.27.mlstm_layer.q.weight', 'backbone.blocks.27.mlstm_layer.v.weight', 'backbone.blocks.27.norm_ffn.weight', 'backbone.blocks.27.norm_mlstm.weight', 'backbone.blocks.28.ffn.proj_down.weight', 'backbone.blocks.28.ffn.proj_up.weight', 'backbone.blocks.28.ffn.proj_up_gate.weight', 'backbone.blocks.28.mlstm_layer.fgate_preact.bias', 'backbone.blocks.28.mlstm_layer.fgate_preact.weight', 'backbone.blocks.28.mlstm_layer.igate_preact.bias', 'backbone.blocks.28.mlstm_layer.igate_preact.weight', 'backbone.blocks.28.mlstm_layer.k.weight', 'backbone.blocks.28.mlstm_layer.multihead_norm.weight', 'backbone.blocks.28.mlstm_layer.ogate_preact.weight', 'backbone.blocks.28.mlstm_layer.out_proj.weight', 'backbone.blocks.28.mlstm_layer.q.weight', 'backbone.blocks.28.mlstm_layer.v.weight', 'backbone.blocks.28.norm_ffn.weight', 'backbone.blocks.28.norm_mlstm.weight', 'backbone.blocks.29.ffn.proj_down.weight', 'backbone.blocks.29.ffn.proj_up.weight', 'backbone.blocks.29.ffn.proj_up_gate.weight', 'backbone.blocks.29.mlstm_layer.fgate_preact.bias', 'backbone.blocks.29.mlstm_layer.fgate_preact.weight', 'backbone.blocks.29.mlstm_layer.igate_preact.bias', 'backbone.blocks.29.mlstm_layer.igate_preact.weight', 'backbone.blocks.29.mlstm_layer.k.weight', 'backbone.blocks.29.mlstm_layer.multihead_norm.weight', 'backbone.blocks.29.mlstm_layer.ogate_preact.weight', 'backbone.blocks.29.mlstm_layer.out_proj.weight', 'backbone.blocks.29.mlstm_layer.q.weight', 'backbone.blocks.29.mlstm_layer.v.weight', 'backbone.blocks.29.norm_ffn.weight', 'backbone.blocks.29.norm_mlstm.weight', 'backbone.blocks.30.ffn.proj_down.weight', 'backbone.blocks.30.ffn.proj_up.weight', 'backbone.blocks.30.ffn.proj_up_gate.weight', 'backbone.blocks.30.mlstm_layer.fgate_preact.bias', 'backbone.blocks.30.mlstm_layer.fgate_preact.weight', 'backbone.blocks.30.mlstm_layer.igate_preact.bias', 'backbone.blocks.30.mlstm_layer.igate_preact.weight', 'backbone.blocks.30.mlstm_layer.k.weight', 'backbone.blocks.30.mlstm_layer.multihead_norm.weight', 'backbone.blocks.30.mlstm_layer.ogate_preact.weight', 'backbone.blocks.30.mlstm_layer.out_proj.weight', 'backbone.blocks.30.mlstm_layer.q.weight', 'backbone.blocks.30.mlstm_layer.v.weight', 'backbone.blocks.30.norm_ffn.weight', 'backbone.blocks.30.norm_mlstm.weight', 'backbone.blocks.31.ffn.proj_down.weight', 'backbone.blocks.31.ffn.proj_up.weight', 'backbone.blocks.31.ffn.proj_up_gate.weight', 'backbone.blocks.31.mlstm_layer.fgate_preact.bias', 'backbone.blocks.31.mlstm_layer.fgate_preact.weight', 'backbone.blocks.31.mlstm_layer.igate_preact.bias', 'backbone.blocks.31.mlstm_layer.igate_preact.weight', 'backbone.blocks.31.mlstm_layer.k.weight', 'backbone.blocks.31.mlstm_layer.multihead_norm.weight', 'backbone.blocks.31.mlstm_layer.ogate_preact.weight', 'backbone.blocks.31.mlstm_layer.out_proj.weight', 'backbone.blocks.31.mlstm_layer.q.weight', 'backbone.blocks.31.mlstm_layer.v.weight', 'backbone.blocks.31.norm_ffn.weight', 'backbone.blocks.31.norm_mlstm.weight']\n",
      "- This IS expected if you are initializing xLSTMForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing xLSTMForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of xLSTMForCausalLM were not initialized from the model checkpoint at /common-repos/xLSTM-7b and are newly initialized because the shapes did not match:\n",
      "- backbone.blocks.0.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.0.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.0.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.0.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.1.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.1.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.1.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.1.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.2.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.2.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.2.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.2.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.3.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.3.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.3.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.3.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.4.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.4.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.4.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.4.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.embeddings.weight: found shape torch.Size([50304, 4096]) in the checkpoint and torch.Size([51200, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.10.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.10.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.10.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.11.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.5.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.5.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.5.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.6.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.6.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.6.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.6.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.7.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.7.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.7.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.7.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.8.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.8.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.8.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.8.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.9.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.9.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.9.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.9.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.11.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.11.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.11.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.12.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.12.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.12.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.13.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.13.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.13.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.13.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.14.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.14.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.14.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.14.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.15.ffn.proj_down.weight: found shape torch.Size([4096, 10944]) in the checkpoint and torch.Size([2048, 5504]) in the model instantiated\n",
      "- backbone.blocks.15.ffn.proj_up.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.ffn.proj_up_gate.weight: found shape torch.Size([10944, 4096]) in the checkpoint and torch.Size([5504, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.fgate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.igate_preact.weight: found shape torch.Size([8, 4096]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.k.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.multihead_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.ogate_preact.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.out_proj.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.q.weight: found shape torch.Size([2048, 4096]) in the checkpoint and torch.Size([1024, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.mlstm_layer.v.weight: found shape torch.Size([4096, 4096]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- backbone.blocks.15.norm_ffn.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.blocks.15.norm_mlstm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- backbone.out_norm.weight: found shape torch.Size([4096]) in the checkpoint and torch.Size([2048]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([50304, 4096]) in the checkpoint and torch.Size([51200, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "xLSTMForCausalLM(\n",
       "  (backbone): xLSTMModel(\n",
       "    (embeddings): Embedding(51200, 2048)\n",
       "    (blocks): ModuleList(\n",
       "      (0-15): 16 x mLSTMBlock(\n",
       "        (norm_mlstm): RMSNorm()\n",
       "        (mlstm_layer): mLSTMLayer(\n",
       "          (q): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (k): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ogate_preact): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (igate_preact): Linear(in_features=2048, out_features=8, bias=True)\n",
       "          (fgate_preact): Linear(in_features=2048, out_features=8, bias=True)\n",
       "          (ogate_act_fn): Sigmoid()\n",
       "          (mlstm_backend): mLSTMBackend(mLSTMBackendConfig(chunkwise_kernel='chunkwise--triton_xl_chunk', sequence_kernel='native_sequence__triton', step_kernel='triton', mode='inference', chunk_size=64, return_last_states=True, autocast_kernel_dtype='bfloat16', eps=1e-06, inference_state_dtype='float32'))\n",
       "          (multihead_norm): MultiHeadLayerNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (norm_ffn): RMSNorm()\n",
       "        (ffn): FeedForward(\n",
       "          (proj_up_gate): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (proj_up): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (proj_down): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_config = Config.from_json(\"assets/sample_config.json\")\n",
    "model_config = AutoConfig.from_pretrained(\"/common-repos/xLSTM-7b\")\n",
    "model_config.num_blocks = 16\n",
    "model_config.head_dim = 256\n",
    "model_config.num_heads = 8\n",
    "model_config.embedding_dim = 2048\n",
    "model_config.vocab_size = teacher_model.config.vocab_size\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\"/common-repos/xLSTM-7b\", config=model_config, ignore_mismatched_sizes=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#student_model.apply(init_weights)\n",
    "student_model.to(device)\n",
    "student_model.requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTMForCausalLM(\n",
       "  (backbone): xLSTMModel(\n",
       "    (embeddings): Embedding(51200, 2048)\n",
       "    (blocks): ModuleList(\n",
       "      (0-15): 16 x mLSTMBlock(\n",
       "        (norm_mlstm): RMSNorm()\n",
       "        (mlstm_layer): mLSTMLayer(\n",
       "          (q): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (k): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ogate_preact): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (igate_preact): Linear(in_features=2048, out_features=8, bias=True)\n",
       "          (fgate_preact): Linear(in_features=2048, out_features=8, bias=True)\n",
       "          (ogate_act_fn): Sigmoid()\n",
       "          (mlstm_backend): mLSTMBackend(mLSTMBackendConfig(chunkwise_kernel='chunkwise--triton_xl_chunk', sequence_kernel='native_sequence__triton', step_kernel='triton', mode='inference', chunk_size=64, return_last_states=True, autocast_kernel_dtype='bfloat16', eps=1e-06, inference_state_dtype='float32'))\n",
       "          (multihead_norm): MultiHeadLayerNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (norm_ffn): RMSNorm()\n",
       "        (ffn): FeedForward(\n",
       "          (proj_up_gate): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (proj_up): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (proj_down): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"checkpoints/epoch_5.pt\"\n",
    "checkpoint = torch.load(path, map_location=device, weights_only=True)\n",
    "model_state_to_load = checkpoint['model_state_dict']\n",
    "#start_epoch = checkpoint['epoch']-1\n",
    "#start_idx = checkpoint['idx']-1\n",
    "\n",
    "\n",
    "student_model.load_state_dict(model_state_to_load)\n",
    "student_model.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking for NaN/Inf in Model Parameters ---\n",
      "No NaN or Inf values found in model parameters. Good!\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming student_model is loaded and on the correct device\n",
    "# student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# student_model.to(device)\n",
    "\n",
    "print(\"--- Checking for NaN/Inf in Model Parameters ---\")\n",
    "found_nan_inf = False\n",
    "for name, param in student_model.named_parameters():\n",
    "    if not torch.all(torch.isfinite(param.data)):\n",
    "        print(f\"WARNING: Parameter '{name}' contains NaN or Inf values!\")\n",
    "        # You can print more details if needed:\n",
    "        # print(f\"  Number of NaNs: {torch.isnan(param.data).sum().item()}\")\n",
    "        # print(f\"  Number of Infs: {torch.isinf(param.data).sum().item()}\")\n",
    "        found_nan_inf = True\n",
    "    # Optional: Check if all values in a parameter are zero (as we did before)\n",
    "    # if param.data.norm().item() == 0.0:\n",
    "    #     print(f\"INFO: Parameter '{name}' is all zeros.\")\n",
    "\n",
    "if not found_nan_inf:\n",
    "    print(\"No NaN or Inf values found in model parameters. Good!\")\n",
    "else:\n",
    "    print(\"CRITICAL: NaN or Inf values detected in loaded weights! This will cause issues.\")\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(next(student_model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding='longest'\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"]\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataloader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_teacher_layers = len(teacher_model.model.layers)\n",
    "num_student_layers = len(student_model.backbone.blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 24\n"
     ]
    }
   ],
   "source": [
    "print(num_student_layers, num_teacher_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = student_model.parameters()\n",
    "optimizer = optim.Adam(params = parameters, lr = 5e-6)\n",
    "\n",
    "vocab_size = teacher_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51200\n"
     ]
    }
   ],
   "source": [
    "print(student_model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_loss(student_logits, teacher_logits, input_ids, epoch, temp=2.0, ce_loss_weight=0.5, soft_target_loss_weight = 0.05, vocab_size = teacher_model.vocab_size ):\n",
    "    \n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        kl_loss_fn = nn.KLDivLoss(reduction = \"batchmean\")\n",
    "        \n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        shift_student_logits = student_logits[:, :-1, :].contiguous()\n",
    "        shift_teacher_logits = teacher_logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        label_loss = ce_loss(shift_student_logits.view(-1, vocab_size), labels.view(-1))\n",
    "        \n",
    "        soft_targets = nn.functional.softmax(shift_teacher_logits/temp, dim = -1)\n",
    "        soft_probs = nn.functional.log_softmax(shift_student_logits/temp, dim = -1)\n",
    "        \n",
    "        if epoch > 2:\n",
    "                soft_targets_loss = kl_loss_fn(soft_probs, soft_targets)*(temp ** 2)\n",
    "        else: \n",
    "                soft_targets_loss = 0.0\n",
    "        \n",
    "        loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "       # print(f\"Iter: {idx}, Current loss: {loss.item()}, CE Loss: {label_loss.item()}, KL Loss: {soft_targets_loss.item()}\")\n",
    "        return loss, label_loss, soft_targets_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoints_without_hidden = \"./checkpoints_logits\"\n",
    "\n",
    "os.makedirs(checkpoints_without_hidden, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch/Iteration: 0 / 0 | Effective Batch Loss: 5.4036 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 1 | Effective Batch Loss: 5.3874 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 2 | Effective Batch Loss: 5.3771 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 3 | Effective Batch Loss: 5.3602 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 4 | Effective Batch Loss: 5.3517 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 5 | Effective Batch Loss: 5.3505 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 6 | Effective Batch Loss: 5.3565 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 7 | Effective Batch Loss: 5.3290 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 8 | Effective Batch Loss: 5.3450 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 9 | Effective Batch Loss: 5.2953 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 10 | Effective Batch Loss: 5.2561 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 11 | Effective Batch Loss: 5.2904 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 12 | Effective Batch Loss: 5.2653 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 13 | Effective Batch Loss: 5.2722 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 14 | Effective Batch Loss: 5.2683 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 15 | Effective Batch Loss: 5.2978 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 16 | Effective Batch Loss: 5.2589 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 17 | Effective Batch Loss: 5.2076 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 18 | Effective Batch Loss: 5.2055 ...\n",
      "\n",
      "Epoch/Iteration: 0 / 19 | Effective Batch Loss: 5.1613 ...\n",
      "\n",
      "hello\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m     checkpoint_path = os.path.join(checkpoints_without_hidden, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(5)epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33midx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m     loss = loss.item() * accumulation_steps\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43midx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moptimizer_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     65\u001b[39m optimizer.step()\n\u001b[32m     66\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py:651\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    648\u001b[39m _check_save_filelike(f)\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py:499\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    501\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "accumulation_steps = 16\n",
    "\n",
    "freeze_mlp = True\n",
    "student_model.to(device)\n",
    "num_epochs = 10\n",
    "alpha = 0.3\n",
    "\n",
    "start_epoch = 0\n",
    "start_idx = 0\n",
    "# sys.exit() # Uncomment to stop after this test\n",
    "with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        student_model.train()\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            if idx < start_idx:\n",
    "                continue\n",
    "            start_idx = 0\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            \n",
    "    \n",
    "            student_outputs = student_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=False,\n",
    "                    use_cache=False,\n",
    "                    output_attention_results=False\n",
    "                )\n",
    "                \n",
    "            student_logits = student_outputs.logits\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=False,\n",
    "                    use_cache=False,\n",
    "                    output_attention_results=False\n",
    "                )\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "            \n",
    "            loss, loss_CE, loss_KL = logit_loss(student_logits, teacher_logits, input_ids, epoch)\n",
    "            \n",
    "            loss = loss / accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "           \n",
    "            if  (idx+1) % accumulation_steps == 0 or idx ==0:\n",
    "                    \n",
    "                    if (idx+1) % 320 == 0:\n",
    "                        print(\"hello\")\n",
    "                        checkpoint_path = os.path.join(checkpoints_without_hidden, f\"(5)epoch_{epoch}idx_{idx}.pt\")\n",
    "                        loss = loss.item() * accumulation_steps\n",
    "                        torch.save({'idx': idx + 1,\n",
    "                        'epoch': epoch + 1, \n",
    "                        'model_state_dict': student_model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        }, checkpoint_path) \n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "                    print(f\"Epoch/Iteration: {epoch} / { (idx + 1) // accumulation_steps } | Effective Batch Loss: {loss.item() * accumulation_steps:.4f} ...\\n\")\n",
    "        \n",
    "        checkpoint_path = os.path.join(checkpoints_without_hidden, f\"epoch_{epoch+1}.pt\")\n",
    "    \n",
    "        torch.save({'epoch': epoch + 1, \n",
    "                    'model_state_dict': student_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    }, checkpoint_path) \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokenizer vocab size: 50257, len(tokenizer): 50295\n",
      "\n",
      "--- Loading Student Model (microsoft/phi-1_5) on cuda:1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original student model vocab size: 51200\n",
      "Resizing student model from 51200 to DEBUG_VOCAB_SIZE 100\n",
      "Replacing lm_head for student model.\n",
      "Student model configured with DEBUG_VOCAB_SIZE: 100\n",
      "Student LM head output features: 100\n",
      "\n",
      "--- Preparing Data ---\n",
      "Lenght of dataloader is: 1250\n",
      "Epoch: 1/10, Iter: 0/1250 | Loss (CE on 100 logits): 4.8763\n",
      "Epoch: 1/10, Iter: 10/1250 | Loss (CE on 100 logits): 0.6374\n",
      "Epoch: 1/10, Iter: 20/1250 | Loss (CE on 100 logits): 0.5382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    151\u001b[39m     optimizer.step()\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m idx % LOG_INTERVAL == \u001b[32m0\u001b[39m:\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Iter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss (CE on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEBUG_VOCAB_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m logits): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    157\u001b[39m     loss_per_epoch[idx] = loss.item()\n\u001b[32m    160\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- End of Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from modules.modeling_phi import PhiForCausalLM # Your custom model\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda:1\"\n",
    "TOKENIZER_NAME = \"microsoft/phi-1_5\"\n",
    "STUDENT_MODEL_NAME = \"microsoft/phi-1_5\"\n",
    "# For this test, we won't use a teacher model or KL loss.\n",
    "DATASET_NAME = \"stas/openwebtext-10k\"\n",
    "MAX_SEQ_LENGTH = 64 # Keep sequences short for faster iteration in this test\n",
    "BATCH_SIZE = 8      # Small batch size for this test\n",
    "LEARNING_RATE = 5e-5 # A slightly higher LR might be okay for a smaller problem\n",
    "NUM_EPOCHS = 10      # Just a few epochs for testing\n",
    "LOG_INTERVAL = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 # No accumulation for simplicity here, unless BATCH_SIZE is 1\n",
    "\n",
    "# ---- MODIFICATION FOR \"FEW LOGITS\" TRAINING ----\n",
    "DEBUG_VOCAB_SIZE = 100 # Train on the first 50 token IDs only\n",
    "# -------------------------------------------------\n",
    "\n",
    "# --- Initialize Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "actual_tokenizer_vocab_size = tokenizer.vocab_size # Store original before potential modification\n",
    "print(f\"Original tokenizer vocab size: {actual_tokenizer_vocab_size}, len(tokenizer): {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "# --- Load Student Model ---\n",
    "print(f\"\\n--- Loading Student Model ({STUDENT_MODEL_NAME}) on {DEVICE} ---\")\n",
    "student_model_full_vocab = PhiForCausalLM.from_pretrained(\n",
    "    STUDENT_MODEL_NAME,\n",
    "    attn_implementation=\"eager\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "original_student_vocab_size = student_model_full_vocab.config.vocab_size\n",
    "print(f\"Original student model vocab size: {original_student_vocab_size}\")\n",
    "\n",
    "# --- !!! TEMPORARILY RESIZE STUDENT MODEL'S VOCAB FOR DEBUGGING !!! ---\n",
    "# This will reinitialize the lm_head and token embeddings for the new size.\n",
    "# We are effectively creating a new, smaller model head.\n",
    "if original_student_vocab_size != DEBUG_VOCAB_SIZE:\n",
    "    print(f\"Resizing student model from {original_student_vocab_size} to DEBUG_VOCAB_SIZE {DEBUG_VOCAB_SIZE}\")\n",
    "    student_model_full_vocab.resize_token_embeddings(DEBUG_VOCAB_SIZE)\n",
    "    # The lm_head (output layer) should also be adjusted.\n",
    "    # For Phi, if lm_head is tied or is student_model.embed_out, resize_token_embeddings handles it.\n",
    "    # If it's a separate nn.Linear called 'lm_head', we might need to replace it.\n",
    "    if hasattr(student_model_full_vocab, 'lm_head') and isinstance(student_model_full_vocab.lm_head, nn.Linear):\n",
    "        print(\"Replacing lm_head for student model.\")\n",
    "        hidden_size = student_model_full_vocab.lm_head.in_features\n",
    "        student_model_full_vocab.lm_head = nn.Linear(hidden_size, DEBUG_VOCAB_SIZE, bias=False) # Or existing bias setting\n",
    "    elif hasattr(student_model_full_vocab, 'embed_out') and isinstance(student_model_full_vocab.embed_out, nn.Linear): # For some Phi versions\n",
    "        print(\"Replacing embed_out (LM head) for student model.\")\n",
    "        hidden_size = student_model_full_vocab.embed_out.in_features\n",
    "        student_model_full_vocab.embed_out = nn.Linear(hidden_size, DEBUG_VOCAB_SIZE, bias=False)\n",
    "    else:\n",
    "        print(\"LM head assumed to be tied or handled by resize_token_embeddings for student.\")\n",
    "    # Update the config to reflect the new vocab size\n",
    "    student_model_full_vocab.config.vocab_size = DEBUG_VOCAB_SIZE\n",
    "else:\n",
    "    print(\"Student model already has the DEBUG_VOCAB_SIZE. No resize needed.\")\n",
    "\n",
    "student_model = student_model_full_vocab.to(DEVICE) # Now move the modified model\n",
    "student_model.train()\n",
    "student_model.requires_grad_(True)\n",
    "print(f\"Student model configured with DEBUG_VOCAB_SIZE: {student_model.config.vocab_size}\")\n",
    "if hasattr(student_model, 'lm_head') and isinstance(student_model.lm_head, nn.Linear):\n",
    "    print(f\"Student LM head output features: {student_model.lm_head.out_features}\")\n",
    "\n",
    "\n",
    "# --- Dataset and DataLoader ---\n",
    "print(\"\\n--- Preparing Data ---\")\n",
    "dataset = load_dataset(DATASET_NAME)[\"train\"].select(range(10000)) # Use a small subset of data for faster testing\n",
    "\n",
    "def collate_fn_debug_vocab(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding='longest',\n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    # --- !!! FILTER/CLAMP TOKEN IDS FOR DEBUGGING !!! ---\n",
    "    # Ensure input_ids and thus labels are within [0, DEBUG_VOCAB_SIZE - 1]\n",
    "    # Method 1: Clamp (simpler, but changes token meaning)\n",
    "    encodings[\"input_ids\"] = torch.clamp(encodings[\"input_ids\"], max=DEBUG_VOCAB_SIZE - 1)\n",
    "    # Method 2: Filter batches (more complex, might lead to very few usable batches)\n",
    "    #   - You'd check if all tokens in encodings[\"input_ids\"] are < DEBUG_VOCAB_SIZE.\n",
    "    #   - This is often too restrictive. Clamping is easier for this specific test.\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_debug_vocab, num_workers=2)\n",
    "print(f\"Lenght of dataloader is: {len(dataloader)}\")\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- CE Loss Function (No KL) ---\n",
    "# This function assumes student_logits already have the reduced vocab dimension\n",
    "def ce_loss_for_debug_vocab(student_logits_reduced_vocab, input_ids_clamped):\n",
    "    ce_loss_criterion = nn.CrossEntropyLoss()\n",
    "    # Labels are derived from input_ids_clamped, so they are also within the reduced vocab\n",
    "    labels = input_ids_clamped[:, 1:].contiguous()\n",
    "    shift_student_logits = student_logits_reduced_vocab[:, :-1, :].contiguous() # Logits are already reduced\n",
    "\n",
    "    calculated_ce_loss = ce_loss_criterion(\n",
    "        shift_student_logits.view(-1, DEBUG_VOCAB_SIZE), # Last dim is DEBUG_VOCAB_SIZE\n",
    "        labels.view(-1)\n",
    "    )\n",
    "    return calculated_ce_loss\n",
    "\n",
    "# --- Training Loop ---\n",
    "#print(f\"\\n--- Starting DEBUG Training (Vocab Size: {DEBUG_VOCAB_SIZE}) for {NUM_EPOCHS} epochs ---\")\n",
    "losses = torch.zeros(NUM_EPOCHS)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    student_model.train()\n",
    "    loss_per_epoch = torch.zeros(len(dataloader))\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids_clamped = batch[\"input_ids\"].to(DEVICE) # Already clamped in collate_fn\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        # Student forward pass - model now outputs logits for DEBUG_VOCAB_SIZE\n",
    "        student_outputs = student_model(\n",
    "            input_ids=input_ids_clamped, # Use clamped input_ids\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            use_cache=False,\n",
    "            output_attentions=False\n",
    "        )\n",
    "        student_logits_reduced_vocab = student_outputs.logits # Shape: [B, S, DEBUG_VOCAB_SIZE]\n",
    "        \n",
    "        # Loss calculation (CE Only on reduced vocabulary)\n",
    "        loss = ce_loss_for_debug_vocab(\n",
    "            student_logits_reduced_vocab,\n",
    "            input_ids_clamped\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % LOG_INTERVAL == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{NUM_EPOCHS}, Iter: {idx}/{len(dataloader)} | \"\n",
    "                  f\"Loss (CE on {DEBUG_VOCAB_SIZE} logits): {loss.item():.4f}\")\n",
    "            \n",
    "        loss_per_epoch[idx] = loss.item()\n",
    "\n",
    "  \n",
    "    print(f\"--- End of Epoch {epoch+1} ---\")\n",
    "    losses[epoch] = torch.mean(loss_per_epoch)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "# !!! IMPORTANT: Remember to revert these model/tokenizer changes for actual full training !!!\n",
    "# You would typically load the original student_model again without resizing for the real run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(epochs, values):\n",
    "    \n",
    "    plt.plot(epochs, values.detach().numpy(), marker = 'o')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss function\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2288835/2718204610.py:6: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(0, 10, 1)\n",
    "\n",
    "plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from modules.modeling_phi import PhiForCausalLM # Your custom model\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda:1\"\n",
    "TOKENIZER_NAME = \"microsoft/phi-1_5\"\n",
    "STUDENT_MODEL_NAME = \"microsoft/phi-1_5\"\n",
    "# For this test, we won't use a teacher model or KL loss.\n",
    "DATASET_NAME = \"stas/openwebtext-10k\"\n",
    "MAX_SEQ_LENGTH = 64 # Keep sequences short for faster iteration in this test\n",
    "BATCH_SIZE = 8      # Small batch size for this test\n",
    "LEARNING_RATE = 5e-5 # A slightly higher LR might be okay for a smaller problem\n",
    "NUM_EPOCHS = 10      # Just a few epochs for testing\n",
    "LOG_INTERVAL = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 # No accumulation for simplicity here, unless BATCH_SIZE is 1\n",
    "\n",
    "# ---- MODIFICATION FOR \"FEW LOGITS\" TRAINING ----\n",
    "DEBUG_VOCAB_SIZE = 100 # Train on the first 50 token IDs only\n",
    "# -------------------------------------------------\n",
    "\n",
    "# --- Initialize Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "actual_tokenizer_vocab_size = tokenizer.vocab_size # Store original before potential modification\n",
    "print(f\"Original tokenizer vocab size: {actual_tokenizer_vocab_size}, len(tokenizer): {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "# --- Load Student Model ---\n",
    "print(f\"\\n--- Loading Student Model ({STUDENT_MODEL_NAME}) on {DEVICE} ---\")\n",
    "student_model_full_vocab = PhiForCausalLM.from_pretrained(\n",
    "    STUDENT_MODEL_NAME,\n",
    "    attn_implementation=\"eager\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "original_student_vocab_size = student_model_full_vocab.config.vocab_size\n",
    "print(f\"Original student model vocab size: {original_student_vocab_size}\")\n",
    "\n",
    "# --- !!! TEMPORARILY RESIZE STUDENT MODEL'S VOCAB FOR DEBUGGING !!! ---\n",
    "# This will reinitialize the lm_head and token embeddings for the new size.\n",
    "# We are effectively creating a new, smaller model head.\n",
    "if original_student_vocab_size != DEBUG_VOCAB_SIZE:\n",
    "    print(f\"Resizing student model from {original_student_vocab_size} to DEBUG_VOCAB_SIZE {DEBUG_VOCAB_SIZE}\")\n",
    "    student_model_full_vocab.resize_token_embeddings(DEBUG_VOCAB_SIZE)\n",
    "    # The lm_head (output layer) should also be adjusted.\n",
    "    # For Phi, if lm_head is tied or is student_model.embed_out, resize_token_embeddings handles it.\n",
    "    # If it's a separate nn.Linear called 'lm_head', we might need to replace it.\n",
    "    if hasattr(student_model_full_vocab, 'lm_head') and isinstance(student_model_full_vocab.lm_head, nn.Linear):\n",
    "        print(\"Replacing lm_head for student model.\")\n",
    "        hidden_size = student_model_full_vocab.lm_head.in_features\n",
    "        student_model_full_vocab.lm_head = nn.Linear(hidden_size, DEBUG_VOCAB_SIZE, bias=False) # Or existing bias setting\n",
    "    elif hasattr(student_model_full_vocab, 'embed_out') and isinstance(student_model_full_vocab.embed_out, nn.Linear): # For some Phi versions\n",
    "        print(\"Replacing embed_out (LM head) for student model.\")\n",
    "        hidden_size = student_model_full_vocab.embed_out.in_features\n",
    "        student_model_full_vocab.embed_out = nn.Linear(hidden_size, DEBUG_VOCAB_SIZE, bias=False)\n",
    "    else:\n",
    "        print(\"LM head assumed to be tied or handled by resize_token_embeddings for student.\")\n",
    "    # Update the config to reflect the new vocab size\n",
    "    student_model_full_vocab.config.vocab_size = DEBUG_VOCAB_SIZE\n",
    "else:\n",
    "    print(\"Student model already has the DEBUG_VOCAB_SIZE. No resize needed.\")\n",
    "\n",
    "student_model = student_model_full_vocab.to(DEVICE) # Now move the modified model\n",
    "student_model.train()\n",
    "student_model.requires_grad_(True)\n",
    "print(f\"Student model configured with DEBUG_VOCAB_SIZE: {student_model.config.vocab_size}\")\n",
    "if hasattr(student_model, 'lm_head') and isinstance(student_model.lm_head, nn.Linear):\n",
    "    print(f\"Student LM head output features: {student_model.lm_head.out_features}\")\n",
    "\n",
    "\n",
    "# --- Dataset and DataLoader ---\n",
    "print(\"\\n--- Preparing Data ---\")\n",
    "dataset = load_dataset(DATASET_NAME)[\"train\"].select(range(10000)) # Use a small subset of data for faster testing\n",
    "\n",
    "def collate_fn_debug_vocab(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding='longest',\n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    # --- !!! FILTER/CLAMP TOKEN IDS FOR DEBUGGING !!! ---\n",
    "    # Ensure input_ids and thus labels are within [0, DEBUG_VOCAB_SIZE - 1]\n",
    "    # Method 1: Clamp (simpler, but changes token meaning)\n",
    "    encodings[\"input_ids\"] = torch.clamp(encodings[\"input_ids\"], max=DEBUG_VOCAB_SIZE - 1)\n",
    "    # Method 2: Filter batches (more complex, might lead to very few usable batches)\n",
    "    #   - You'd check if all tokens in encodings[\"input_ids\"] are < DEBUG_VOCAB_SIZE.\n",
    "    #   - This is often too restrictive. Clamping is easier for this specific test.\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_debug_vocab, num_workers=2)\n",
    "print(f\"Lenght of dataloader is: {len(dataloader)}\")\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- CE Loss Function (No KL) ---\n",
    "# This function assumes student_logits already have the reduced vocab dimension\n",
    "def ce_loss_for_debug_vocab(student_logits_reduced_vocab, input_ids_clamped):\n",
    "    ce_loss_criterion = nn.CrossEntropyLoss()\n",
    "    # Labels are derived from input_ids_clamped, so they are also within the reduced vocab\n",
    "    labels = input_ids_clamped[:, 1:].contiguous()\n",
    "    shift_student_logits = student_logits_reduced_vocab[:, :-1, :].contiguous() # Logits are already reduced\n",
    "\n",
    "    calculated_ce_loss = ce_loss_criterion(\n",
    "        shift_student_logits.view(-1, DEBUG_VOCAB_SIZE), # Last dim is DEBUG_VOCAB_SIZE\n",
    "        labels.view(-1)\n",
    "    )\n",
    "    return calculated_ce_loss\n",
    "\n",
    "# --- Training Loop ---\n",
    "#print(f\"\\n--- Starting DEBUG Training (Vocab Size: {DEBUG_VOCAB_SIZE}) for {NUM_EPOCHS} epochs ---\")\n",
    "losses = torch.zeros(NUM_EPOCHS)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    student_model.train()\n",
    "    loss_per_epoch = torch.zeros(len(dataloader))\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids_clamped = batch[\"input_ids\"].to(DEVICE) # Already clamped in collate_fn\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        # Student forward pass - model now outputs logits for DEBUG_VOCAB_SIZE\n",
    "        student_outputs = student_model(\n",
    "            input_ids=input_ids_clamped, # Use clamped input_ids\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            use_cache=False,\n",
    "            output_attentions=False\n",
    "        )\n",
    "        student_logits_reduced_vocab = student_outputs.logits # Shape: [B, S, DEBUG_VOCAB_SIZE]\n",
    "        \n",
    "        # Loss calculation (CE Only on reduced vocabulary)\n",
    "        loss = ce_loss_for_debug_vocab(\n",
    "            student_logits_reduced_vocab,\n",
    "            input_ids_clamped\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % LOG_INTERVAL == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{NUM_EPOCHS}, Iter: {idx}/{len(dataloader)} | \"\n",
    "                  f\"Loss (CE on {DEBUG_VOCAB_SIZE} logits): {loss.item():.4f}\")\n",
    "            \n",
    "        loss_per_epoch[idx] = loss.item()\n",
    "\n",
    "  \n",
    "    print(f\"--- End of Epoch {epoch+1} ---\")\n",
    "    losses[epoch] = torch.mean(loss_per_epoch)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "# !!! IMPORTANT: Remember to revert these model/tokenizer changes for actual full training !!!\n",
    "# You would typically load the original student_model again without resizing for the real run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
